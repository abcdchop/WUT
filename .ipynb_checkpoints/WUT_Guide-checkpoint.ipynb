{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, Welcome to the Internal Guide to this Uncertainy Library-- This python notebook should contain everything a man needs to use and alter this package as he sees fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this, we are going to use the same datasets for regression and classification. The regression dataset can be found at https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip and the classifiction dataset can be found at https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz. You will obviously need to get these to continue this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import types\n",
    "from tensorflow.python.keras.layers.ops import core as core_ops\n",
    "import mdn\n",
    "from tensorflow.compat.v1.keras import layers\n",
    "from tensorflow.python.keras import activations\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers\n",
    "import tensorflow_probability as tfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515345, 90) (515345,)\n",
      "(581012, 54) (581012, 7)\n"
     ]
    }
   ],
   "source": [
    "'Here we make a Regression Dataset'\n",
    "\n",
    "text_file = open(\"../../../YearPredictionMSD.txt\", \"r\")\n",
    "lines = text_file.readlines()\n",
    "alldata = [[float(x) for x in line.split(',')] for line in lines]\n",
    "text_file.close()\n",
    "alldata = np.array(alldata)\n",
    "Yreg = tf.convert_to_tensor(alldata[:,0], dtype = tf.float32)\n",
    "Xreg = tf.convert_to_tensor(alldata[:,1:], dtype = tf.float32)\n",
    "# Yreg = (Yreg - tf.math.reduce_mean(Yreg))/tf.math.reduce_std(Yreg)\n",
    "# Xreg = (Xreg - tf.math.reduce_mean(Xreg, axis=0, keepdims=True))/tf.math.reduce_std(Xreg, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "# Yreg = Yreg/tf.reduce_max(Yreg)\n",
    "\n",
    "print(Xreg.shape, Yreg.shape)\n",
    "\n",
    "'Here we make a classification Dataset'\n",
    "\n",
    "\n",
    "text_file = open(\"../../../covtype.txt\", \"r\")\n",
    "lines = text_file.readlines()\n",
    "alldata = [[float(x) for x in line.split(',')] for line in lines]\n",
    "text_file.close()\n",
    "alldata = np.array(alldata)\n",
    "ImYclass = alldata[:,-1].astype(int)-1\n",
    "Xclass = (alldata[:,:-1])\n",
    "Yclass = np.zeros((ImYclass.size, ImYclass.max()+1))\n",
    "Yclass[np.arange(ImYclass.size), ImYclass] = 1\n",
    "Yclass = tf.convert_to_tensor(Yclass, dtype = tf.float32)\n",
    "Xclass = tf.convert_to_tensor(Xclass, dtype = tf.float32)\n",
    "\n",
    "print(Xclass.shape, Yclass.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're gonna make two models, very simple bois for regression and classification, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RegModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RegModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(60, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(30, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "    \n",
    "class ClassModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClassModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(36, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(36, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(7, activation = tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could then train these guys normally: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16105/16105 [==============================] - 12s 716us/step - loss: 50605.9258\n",
      "16105/16105 [==============================] - 7s 462us/step - loss: 20316.3926\n",
      "20316.392578125\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffb0a83ae18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[2073.9182]\n",
      " [2105.907 ]\n",
      " [2041.5868]\n",
      " [1959.6626]\n",
      " [2106.3499]]\n",
      "18157/18157 [==============================] - 13s 696us/step - loss: 2.1874\n",
      "18157/18157 [==============================] - 9s 494us/step - loss: 0.8111\n",
      "0.8111305832862854\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffb0ae28158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[5.08685820e-02 9.41301823e-01 2.03644072e-06 1.36389380e-21\n",
      "  7.75355520e-03 7.40167670e-05 7.17037866e-13]\n",
      " [6.34897128e-02 9.28424835e-01 2.27449868e-06 2.12558457e-21\n",
      "  7.98920821e-03 9.39508463e-05 5.88551885e-13]\n",
      " [1.27062500e-01 8.71733904e-01 1.91088861e-06 1.21502717e-14\n",
      "  1.19192514e-03 8.20808236e-06 1.56824558e-06]\n",
      " [2.44414702e-01 7.54212081e-01 3.07503865e-06 2.66788275e-11\n",
      "  1.33916445e-03 1.12530342e-05 1.96840101e-05]\n",
      " [1.06573924e-01 8.87791336e-01 2.09131440e-06 9.73061345e-21\n",
      "  5.51222684e-03 1.20390410e-04 1.87630163e-12]]\n"
     ]
    }
   ],
   "source": [
    "regmodel = RegModel()\n",
    "regmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.MSE)\n",
    "regmodel.fit(Xreg, Yreg)\n",
    "print(regmodel.evaluate(Xreg, Yreg))\n",
    "print(regmodel.predict(Xreg[0:5]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classmodel = ClassModel()\n",
    "classmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy())\n",
    "classmodel.fit(Xclass, Yclass)\n",
    "print(classmodel.evaluate(Xclass, Yclass))\n",
    "print(classmodel.predict(Xclass[0:5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Ensemble():\n",
    "    def __init__(self, Model, num_ens=1):\n",
    "        self.ensemble = [Model() for _ in range(num_ens)]   \n",
    "    def compile(self, *args, **kwargs):\n",
    "        for submodel in self.ensemble:\n",
    "            submodel.compile(*args, **kwargs)        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        for submodel in self.ensemble:\n",
    "            submodel.fit(*args, **kwargs)\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        results = []\n",
    "        for submodel in self.ensemble:\n",
    "            test_scores = submodel.evaluate(*args, **kwargs)\n",
    "            results.append(test_scores)\n",
    "        if type(results[0]) is tuple:\n",
    "            return list(zip(*results))\n",
    "        return results\n",
    "    def predict(self, *args, return_std = True, **kwargs):\n",
    "        predictions = [submodel.predict(*args, **kwargs) for submodel in self.ensemble]\n",
    "        predictions = tf.stack(predictions)\n",
    "        \n",
    "        mean_preds = tf.reduce_mean(predictions, axis = 0)\n",
    "\n",
    "        if not return_std:\n",
    "            return mean_preds\n",
    "        \n",
    "        mean_preds = tf.expand_dims(mean_preds, 1)\n",
    "        std_preds = tf.math.reduce_std(predictions, axis = 0)\n",
    "        std_preds = tf.expand_dims(std_preds, 1)\n",
    "\n",
    "        return tf.concat([mean_preds, std_preds], axis = 1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16105/16105 [==============================] - 11s 693us/step - loss: 49808.4805\n",
      "16105/16105 [==============================] - 13s 830us/step - loss: 37864.3672\n",
      "16105/16105 [==============================] - 12s 732us/step - loss: 17288.1836\n",
      "16105/16105 [==============================] - 10s 611us/step - loss: 16119.0127\n",
      "[17288.18359375, 16119.0126953125]\n",
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffb1ca54d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffb1ca54158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor(\n",
      "[[[2031.3912  ]\n",
      "  [  30.09021 ]]\n",
      "\n",
      " [[2107.0083  ]\n",
      "  [  18.649536]]\n",
      "\n",
      " [[2055.0388  ]\n",
      "  [  15.017639]]\n",
      "\n",
      " [[2030.432   ]\n",
      "  [  11.272217]]\n",
      "\n",
      " [[2137.886   ]\n",
      "  [  35.96997 ]]], shape=(5, 2, 1), dtype=float32)\n",
      "18157/18157 [==============================] - 18s 999us/step - loss: 1.9300\n",
      " 2524/18157 [===>..........................] - ETA: 14s - loss: 4.2214"
     ]
    }
   ],
   "source": [
    "regmodel = Ensemble(RegModel, 2)\n",
    "regmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.MSE)\n",
    "regmodel.fit(Xreg, Yreg)\n",
    "print(regmodel.evaluate(Xreg, Yreg))\n",
    "print(regmodel.predict(Xreg[0:5]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classmodel = Ensemble(ClassModel, 2)\n",
    "classmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy())\n",
    "classmodel.fit(Xclass, Yclass)\n",
    "print(classmodel.evaluate(Xclass, Yclass))\n",
    "print(classmodel.predict(Xclass[0:5]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dropout():\n",
    "    def __init__(self, Model, rate, dropout_layers = [tf.keras.layers.Dense]):\n",
    "        self.model = Model()\n",
    "        \n",
    "        def adddropout(denselayer):\n",
    "            def func(self, inputs, **kwargs):\n",
    "                x = core_ops.dense(inputs, self.kernel, self.bias, self.activation, dtype=self._compute_dtype_object)\n",
    "                return tf.nn.dropout(x, noise_shape=None, rate=rate)\n",
    "            return func\n",
    "        \n",
    "        for layer in self.model.layers[:-1]:\n",
    "            if layer.__class__ in dropout_layers:\n",
    "                func = adddropout(layer)\n",
    "                layer.call = types.MethodType(func, layer)\n",
    "                \n",
    "    def compile(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)        \n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        \n",
    "    def evaluate(self, *args, trials = 3, **kwargs):\n",
    "        results = []\n",
    "        for _ in range(trials):\n",
    "            test_scores = self.model.evaluate(*args, **kwargs)\n",
    "            results.append(test_scores)\n",
    "        if type(results[0]) is tuple:\n",
    "            return list(zip(*results))\n",
    "        return results\n",
    "\n",
    "    def predict(self, *args, trials = 3, return_std = True, **kwargs):\n",
    "#         print(*args)\n",
    "        predictions = [self.model.predict(*args, **kwargs) for _ in range(trials)]\n",
    "        predictions = tf.stack(predictions)\n",
    "#         print(predictions.shape)\n",
    "#         print(predictions)\n",
    "        mean_preds = tf.reduce_mean(predictions, axis = 0)\n",
    "\n",
    "        if not return_std:\n",
    "            return mean_preds\n",
    "        \n",
    "        \n",
    "        std_preds = tf.math.reduce_std(predictions, axis = 0)\n",
    "        mean_preds = tf.expand_dims(mean_preds, 1)\n",
    "\n",
    "        std_preds = tf.expand_dims(std_preds, 1)\n",
    "\n",
    "\n",
    "        return tf.concat([mean_preds, std_preds], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regmodel = Dropout(RegModel, 0.2)\n",
    "regmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.MSE)\n",
    "regmodel.fit(Xreg, Yreg)\n",
    "print(regmodel.evaluate(Xreg, Yreg) )\n",
    "print(regmodel.predict(Xreg[0:5], trials = 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classmodel = Dropout(ClassModel, 0.2)\n",
    "classmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy())\n",
    "classmodel.fit(Xclass, Yclass)\n",
    "print(classmodel.evaluate(Xclass, Yclass))\n",
    "print(classmodel.predict(Xclass[0:5], trials = 2) )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Predict():\n",
    "    def __init__(self, Model, Std_Model = None, error_activation = None):\n",
    "        if Std_Model is None:\n",
    "            Std_Model = Model\n",
    "        self.model = Model()\n",
    "        self.std_model = Std_Model()\n",
    "        \n",
    "        self.error_norm = 1\n",
    "        self.error_activation = error_activation\n",
    "        \n",
    "        \n",
    "        \n",
    "        layer = self.std_model.layers[-1]\n",
    "#         print(layer.activation)\n",
    "        layer.activation = activations.get(error_activation)\n",
    "#         print(layer.activation)\n",
    "\n",
    "        \n",
    "    def compile(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "        new_kwargs = kwargs\n",
    "        new_kwargs['loss'] = tf.keras.losses.MSE\n",
    "        self.std_model.compile(*args, **new_kwargs)\n",
    "\n",
    "        \n",
    "        \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)       \n",
    "        preds = self.model.predict(args[0])\n",
    "        preds = preds.reshape(args[1].shape)\n",
    "        errors = ((args[1] - preds)**2)**.5       \n",
    "        new_args = list(args)\n",
    "        new_args[1] = tf.reshape(errors, args[1].shape)\n",
    "        new_args = tuple(new_args)\n",
    "        self.std_model.fit(*new_args, **kwargs)\n",
    "        \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        return self.model.evaluate( *args, **kwargs)\n",
    "   \n",
    "    def predict(self, *args, return_std = True, **kwargs):\n",
    "        mean_preds = self.model.predict(*args, **kwargs)\n",
    "        std_preds = self.std_model.predict(*args, **kwargs)    \n",
    "        mean_preds = tf.expand_dims(mean_preds, 1)\n",
    "        std_preds = (tf.expand_dims(std_preds, 1) * self.error_norm)\n",
    "\n",
    "        if not return_std:\n",
    "            return np.mean(tf.stack(predictions), 0)\n",
    "        return tf.concat([mean_preds, std_preds], axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regmodel = Self_Predict(RegModel)\n",
    "regmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.MSE)\n",
    "regmodel.fit(Xreg, Yreg)\n",
    "print(regmodel.evaluate(Xreg, Yreg)) \n",
    "print(regmodel.predict(Xreg[0:5]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classmodel = Self_Predict(ClassModel)\n",
    "classmodel.compile(optimizer = tf.keras.optimizers.Adam(), loss = tf.keras.losses.CategoricalCrossentropy())\n",
    "classmodel.fit(Xclass, Yclass)\n",
    "print(classmodel.evaluate(Xclass, Yclass))\n",
    "print(classmodel.predict(Xclass[0:5]))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gaussian_Mixtures():\n",
    "    def __init__(self, Model, num_mixtures=1):\n",
    "        self.model = Model()\n",
    "        \n",
    "        \n",
    "        layer = self.model.layers[-1]\n",
    "        \n",
    "        self.output_dim = layer.units\n",
    "        layer.output_dim = layer.units\n",
    "        self.num_mix = num_mixtures\n",
    "        layer.num_mix = num_mixtures\n",
    "        with tf.name_scope('MDN'):\n",
    "            layer.mdn_mus = layers.Dense(layer.num_mix * layer.output_dim, name='mdn_mus')  # mix*output vals, no activation\n",
    "            layer.mdn_sigmas = layers.Dense(self.num_mix * self.output_dim, activation=self.elu_plus_one_plus_epsilon, name='mdn_sigmas')  # mix*output vals exp activation\n",
    "            layer.mdn_pi = layers.Dense(self.num_mix, name='mdn_pi')  # mix vals, logits\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            with tf.name_scope('mus'):\n",
    "                self.mdn_mus.build(input_shape)\n",
    "            with tf.name_scope('sigmas'):\n",
    "                self.mdn_sigmas.build(input_shape)\n",
    "            with tf.name_scope('pis'):\n",
    "                self.mdn_pi.build(input_shape)\n",
    "\n",
    "        def call_func(self, x):\n",
    "            with tf.name_scope('MDN'):\n",
    "                mdn_out = layers.concatenate([self.mdn_mus(x),\n",
    "                                              self.mdn_sigmas(x),\n",
    "                                              self.mdn_pi(x)],\n",
    "                                             name='mdn_outputs')\n",
    "            return mdn_out\n",
    "\n",
    "        \n",
    "        def compute_output_shape(self, input_shape):\n",
    "            \"\"\"Returns output shape, showing the number of mixture parameters.\"\"\"\n",
    "            return (input_shape[0], (2 * self.output_dim * self.num_mix) + self.num_mix)\n",
    "        \n",
    "        def get_config(self):\n",
    "            config = {\n",
    "                \"output_dimension\": self.output_dim,\n",
    "                \"num_mixtures\": self.num_mix\n",
    "            }\n",
    "            base_config = super(Dense, self).get_config()\n",
    "            return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "        layer.build = types.MethodType(build, layer)\n",
    "        layer.call = types.MethodType(call_func, layer)\n",
    "        layer._trainable_weights = layer.mdn_mus.trainable_weights + layer.mdn_sigmas.trainable_weights + layer.mdn_pi.trainable_weights\n",
    "        layer._non_trainable_weights = layer.mdn_mus.non_trainable_weights + layer.mdn_sigmas.non_trainable_weights + layer.mdn_pi.non_trainable_weights\n",
    "        layer.compute_output_shape = types.MethodType(compute_output_shape, layer)\n",
    "        layer.get_config = types.MethodType(get_config, layer)\n",
    "        \n",
    "        \n",
    "    def elu_plus_one_plus_epsilon(self, x):\n",
    "        \"\"\"ELU activation with a very small addition to help prevent\n",
    "        NaN in loss.\"\"\"\n",
    "        return tf.keras.backend.elu(x) + 1 + .00001\n",
    "\n",
    "    def compile(self, *args, loss=None, **kwargs):\n",
    "        if loss is None:\n",
    "            loss = mdn.get_mixture_loss_func(self.output_dim, self.num_mix)\n",
    "        kwargs['loss'] = loss          \n",
    "        self.model.compile(*args, **kwargs)        \n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        return self.model.evaluate( *args, **kwargs)\n",
    "    \n",
    "    \n",
    "    def predict(self, *args, return_std = True, **kwargs):\n",
    "        all_preds = self.model.predict(*args, **kwargs)\n",
    "        return self.get_dist(all_preds)\n",
    "\n",
    "    def get_dist(self, y_pred):\n",
    "        num_mix = self.num_mix\n",
    "        output_dim = self.output_dim\n",
    "        y_pred = tf.reshape(y_pred, [-1, (2 * num_mix * output_dim) + num_mix], name='reshape_ypreds')\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mix * output_dim,\n",
    "                                                                         num_mix * output_dim,\n",
    "                                                                         num_mix],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = tfd.Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mix\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        return tfd.Mixture(cat=cat, components=coll)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# class Mixture():\n",
    "#     def __init__(self, y_pred, num_mix, output_dim):\n",
    "#         self.out_dist = y_pred\n",
    "#         self.num_mix = num_mix\n",
    "#         self.output_dim = output_dim\n",
    "        \n",
    "#         y_pred = tf.reshape(y_pred, [-1, (2 * num_mix * output_dim) + num_mix], name='reshape_ypreds')\n",
    "#         out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mix * output_dim,\n",
    "#                                                                          num_mix * output_dim,\n",
    "#                                                                          num_mix],\n",
    "#                                              axis=1, name='mdn_coef_split')\n",
    "#         cat = tfd.Categorical(logits=out_pi)\n",
    "#         component_splits = [output_dim] * num_mix\n",
    "#         mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "#         sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "#         coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "#                 in zip(mus, sigs)]\n",
    "#         self.mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "        \n",
    "        \n",
    "#     def sample(self):\n",
    "#         return self.mixture.sample()\n",
    "    \n",
    "#     def mse(self, y_true):\n",
    "#         samp = self.mixture.sample()\n",
    "#         y_true = tf.reshape(y_true, [-1, self.output_dim], name='reshape_ytrue')\n",
    "#         mse = tf.reduce_mean(tf.square(samp - y_true), axis=-1)\n",
    "#         # Todo: temperature adjustment for sampling functon.\n",
    "#         return mse\n",
    "    \n",
    "#     def log_likelihood(self, y_true):\n",
    "#         y_true = tf.reshape(y_true, [-1, self.output_dim], name='reshape_ytrue')\n",
    "#         return self.mixture.log_prob(y_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 904us/step - loss: 37651701760.0000\n",
      "37651701760.0\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa7d145cd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "tf.Tensor(\n",
      "[[-1.83145058e+00 -7.98284607e+01  4.14751589e-01 -1.38629242e+02\n",
      "  -4.64119225e+01 -1.76372206e+00  9.31161041e+01]\n",
      " [ 2.77067947e+00 -1.41249710e+02  2.63736629e+00 -1.61776779e+02\n",
      "  -2.12485294e+01  7.69042611e-01 -3.10771141e+01]\n",
      " [ 1.40198410e+02 -2.64096985e+02  1.07798442e-01  2.23764465e+02\n",
      "  -1.29396808e+00 -1.90287679e-01 -7.29819477e-01]\n",
      " [-3.75903778e+01 -4.78315491e+02 -1.05292045e-01  3.53546631e+02\n",
      "   1.30463421e+00  1.88017413e-01  7.29207218e-01]\n",
      " [-9.36701477e-01 -5.65424271e+01 -3.07723498e+00 -7.78165665e+01\n",
      "   9.32977066e+01  1.00567365e+00  8.02884521e+01]], shape=(5, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# regmodel = Gaussian_Mixtures(RegModel, 5)\n",
    "# regmodel.compile(optimizer = tf.keras.optimizers.Adam())\n",
    "# regmodel.fit(Xreg, Yreg)\n",
    "# print(regmodel.evaluate(Xreg, Yreg)) \n",
    "# regression_dist = regmodel.predict(Xreg[0:5])\n",
    "# print(regression_dist.sample())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classmodel = Gaussian_Mixtures(ClassModel, 5)\n",
    "classmodel.compile(optimizer = tf.keras.optimizers.Adam())\n",
    "classmodel.fit(Xclass[0:5], Yclass[0:5], epochs = 1000, verbose = False)\n",
    "print(classmodel.evaluate(Xclass[0:5], Yclass[0:5]))\n",
    "classification_dist = classmodel.predict(Xclass[0:5])\n",
    "print(classification_dist.sample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]], shape=(5, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(Yclass[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI():\n",
    "    def __init__(self, Model, kl_weight=1., prior_sigma_1=1.5, prior_sigma_2=0.1, prior_pi=0.5, SVI_Layers=[tf.keras.layers.Dense], max_var = .8, min_var=.2, normalize = True, task = 'regression', one_hot = False, **kwargs):\n",
    "                \n",
    "        self.model = Model()\n",
    "        \n",
    "        self.SVI_Layers = SVI_Layers\n",
    "        \n",
    "        self.train_std = 0\n",
    "        \n",
    "        self.max_var = max_var\n",
    "        \n",
    "        self.min_var = min_var\n",
    "        \n",
    "        self.one_hot = one_hot\n",
    "        \n",
    "        \n",
    "        self.xmean, self.xstd = 0., 1.\n",
    "        \n",
    "        self.ymean, self.ystd = 0., 1.\n",
    "        \n",
    "        self.use_normalization = normalize\n",
    "\n",
    "\n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return input_shape[0], self.units\n",
    "        \n",
    "        self.task = task\n",
    "        \n",
    "        if task == 'regression':\n",
    "            last_layer = self.model.layers[-1]\n",
    "            self.dim = last_layer.units\n",
    "            last_layer.units = 2 * last_layer.units\n",
    "            \n",
    "            \n",
    "        if task == 'classification':\n",
    "            self.dim = self.model.layers[-1].units\n",
    "\n",
    "\n",
    "        def kl_loss(self, w, mu, sigma):\n",
    "            variational_dist = tfp.distributions.Normal(mu, sigma)\n",
    "            return self.kl_weight * K.sum(variational_dist.log_prob(w) - self.log_prior_prob(w))\n",
    "\n",
    "\n",
    "            \n",
    "        def build(self, input_shape):\n",
    "            print(\"WE BUILD\")\n",
    "            self.kernel_mu = self.add_weight(name='kernel_mu',\n",
    "                                             shape=(input_shape[1], self.units),\n",
    "                                             initializer=initializers.RandomNormal(stddev=self.init_sigma),\n",
    "                                             trainable=True)\n",
    "            self.bias_mu = self.add_weight(name='bias_mu',\n",
    "                                           shape=(self.units,),\n",
    "                                           initializer=initializers.RandomNormal(stddev=self.init_sigma),\n",
    "                                           trainable=True)\n",
    "            self.kernel_rho = self.add_weight(name='kernel_rho',\n",
    "                                              shape=(input_shape[1], self.units),\n",
    "                                              initializer=initializers.Constant(0.0),\n",
    "                                              trainable=True)\n",
    "            self.bias_rho = self.add_weight(name='bias_rho',\n",
    "                                            shape=(self.units,),\n",
    "                                            initializer=initializers.Constant(0.0),\n",
    "                                            trainable=True)\n",
    "            self._trainable_weights = [self.kernel_mu, self.bias_mu, self.kernel_rho, self.bias_rho]# \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "        def call(self, inputs, **kwargs):\n",
    "            \n",
    "            if self.built == False:\n",
    "                self.build(inputs.shape)\n",
    "                self.built = True\n",
    "            \n",
    "            \n",
    "            kernel_sigma = tf.math.softplus(self.kernel_rho)\n",
    "            kernel = self.kernel_mu + kernel_sigma * tf.random.normal(self.kernel_mu.shape)\n",
    "\n",
    "            bias_sigma = tf.math.softplus(self.bias_rho)\n",
    "            bias = self.bias_mu + bias_sigma * tf.random.normal(self.bias_mu.shape)\n",
    "\n",
    "            self.add_loss(self.kl_loss(kernel, self.kernel_mu, kernel_sigma) +\n",
    "                          self.kl_loss(bias, self.bias_mu, bias_sigma))\n",
    "\n",
    "            return self.activation(K.dot(inputs, kernel) + bias)\n",
    "\n",
    "\n",
    "        def log_prior_prob(self, w):\n",
    "            comp_1_dist = tfp.distributions.Normal(0.0, self.prior_sigma_1)\n",
    "            comp_2_dist = tfp.distributions.Normal(0.0, self.prior_sigma_2)\n",
    "            return K.log(self.prior_pi_1 * comp_1_dist.prob(w) +\n",
    "                         self.prior_pi_2 * comp_2_dist.prob(w))\n",
    "\n",
    "\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if layer.__class__ in SVI_Layers: \n",
    "                print(\"HELLO\")\n",
    "                layer.kl_weight = kl_weight\n",
    "                print(layer.kl_weight)\n",
    "                layer.prior_sigma_1 = prior_sigma_1\n",
    "                layer.prior_sigma_2 = prior_sigma_2\n",
    "                layer.prior_pi_1 = prior_pi\n",
    "                layer.prior_pi_2 = 1.0 - prior_pi\n",
    "                layer.init_sigma = np.sqrt(layer.prior_pi_1 * layer.prior_sigma_1 ** 2 +\n",
    "                                          layer.prior_pi_2 * layer.prior_sigma_2 ** 2)\n",
    "\n",
    "                layer.compute_output_shape = types.MethodType(compute_output_shape, layer)\n",
    "                layer.build = types.MethodType(build, layer)\n",
    "                layer.call = types.MethodType(call, layer)\n",
    "                layer.kl_loss = types.MethodType(kl_loss, layer)\n",
    "                layer.log_prior_prob = types.MethodType(log_prior_prob, layer)\n",
    "                layer.built = False\n",
    "                \n",
    "                \n",
    "                \n",
    "    def fit_normalize(self, X):\n",
    "        mean = tf.math.reduce_mean(X, axis=0, keepdims=True)\n",
    "        std = tf.math.reduce_std(X, axis=0, keepdims=True)\n",
    "        return (X - mean)/std, mean, std\n",
    "    \n",
    "    def unnormalize(self, Y):\n",
    "        return (Y*self.ystd) + self.ymean\n",
    "    \n",
    "    def Y_normalize(self, Y):\n",
    "        return (Y - self.ymean)/self.ystd\n",
    "    \n",
    "    def std_unnormalize(self, Y):\n",
    "        return Y * self.ystd\n",
    "    \n",
    "    def normalize(self, X):\n",
    "        return (X - self.xmean)/self.xstd\n",
    "        \n",
    "        \n",
    "\n",
    "    def compile(self, *args, loss=None, **kwargs):\n",
    "        if loss is None:\n",
    "            loss = self.neg_log_likelihood\n",
    "        kwargs['loss'] = loss          \n",
    "        self.model.compile(*args, **kwargs)   \n",
    "        \n",
    "        \n",
    "        \n",
    "    def elu_plus_one_plus_epsilon(self, x):\n",
    "        \"\"\"ELU activation with a very small addition to help prevent\n",
    "        NaN in loss.\"\"\"\n",
    "        return tf.keras.backend.elu(x) + 1 + .00001\n",
    "\n",
    "\n",
    "        \n",
    "    def neg_log_likelihood(self, y_obs, y_pred):\n",
    "        if self.task == 'regression':\n",
    "            y_means, y_stds = tf.split(y_pred, [self.dim, self.dim], axis = 1)\n",
    "            if self.train_std == 1:\n",
    "                y_stds = ((self.max_var - .1) * tf.math.sigmoid(y_stds)) + .1\n",
    "            else:\n",
    "                y_stds = tf.constant(1.0)\n",
    "            dist = tfp.distributions.Normal(loc=y_means, scale=y_stds )\n",
    "            return K.sum(-dist.log_prob(tf.dtypes.cast(y_obs, tf.int32)))\n",
    "        \n",
    "        \n",
    "        if self.task == 'classification':\n",
    "            y_pred = tf.reshape(y_pred, [-1, self.dim])\n",
    "            y_obs = tf.reshape(y_obs, [-1, self.dim, 1])\n",
    "            dist = tfp.distributions.Categorical(logits = y_pred)\n",
    "#             return tf.reduce_sum(y_pred)\n",
    "            return K.sum(-dist.log_prob(tf.dtypes.cast(y_obs, tf.int32)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        if self.use_normalization is True:\n",
    "            args = list(args)\n",
    "            args[0] = self.normalize(args[0])\n",
    "            if self.task == 'regression':\n",
    "                args[1] = self.Y_normalize(args[1])\n",
    "            args = tuple(args)\n",
    "        return self.model.evaluate( *args, **kwargs)\n",
    "    \n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        if self.use_normalization is True:\n",
    "            args = list(args)\n",
    "            args[0], self.xmean, self.xstd = self.fit_normalize(args[0])\n",
    "            if self.task == 'regression':\n",
    "                args[1], self.ymean, self.ystd = self.fit_normalize(args[1])\n",
    "            args = tuple(args)\n",
    "        \n",
    "        if 'batch_size' not in kwargs.keys():\n",
    "            kwargs['batch_size'] = 32\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if layer.__class__ in self.SVI_Layers:            \n",
    "                layer.kl_weight = kwargs['batch_size'] * layer.kl_weight/args[0].shape[0]\n",
    "                \n",
    "        if (self.one_hot == False) and self.task == 'classification':\n",
    "            args = list(args)\n",
    "            args[1] = tf.one_hot(y_obs, tf.dtypes.cast(tf.reduce_max(y_obs), tf.int32))\n",
    "            args = tuple(args)\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        \n",
    "        self.train_std = 1\n",
    "        \n",
    "        self.model.fit(*args, **kwargs)\n",
    "        \n",
    "        self.train_std = 0\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if layer.__class__ in self.SVI_Layers:            \n",
    "                layer.kl_weight =  layer.kl_weight*args[0].shape[0]/kwargs['batch_size']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, *args, return_std = True, **kwargs):\n",
    "        if self.use_normalization is True:\n",
    "            args = list(args)\n",
    "            args[0] = self.normalize(args[0])\n",
    "            args = tuple(args)\n",
    "        y_pred = self.model.predict(*args, **kwargs)\n",
    "        \n",
    "        if self.task == 'regression':\n",
    "            y_means, y_stds = tf.split(y_pred, [self.dim, self.dim], axis = 1)\n",
    "            if self.use_normalization is True:\n",
    "                y_means = self.unnormalize(y_means)\n",
    "                y_stds = self.std_unnormalize(tf.exp(y_stds))\n",
    "            dist = tfp.distributions.Normal(loc=y_means, scale=y_stds)\n",
    "            return dist\n",
    "        \n",
    "        if self.task == 'classification':\n",
    "            y_pred = tf.reshape(y_pred,  [-1, self.dim])\n",
    "            dist = tfp.distributions.Categorical(logits= y_pred)\n",
    "            return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n",
      "1\n",
      "HELLO\n",
      "1\n",
      "HELLO\n",
      "1\n",
      "WE BUILD\n",
      "WE BUILD\n",
      "WE BUILD\n",
      "18157/18157 [==============================] - 43s 2ms/step - loss: 9485.9922\n",
      " 8813/18157 [=============>................] - ETA: 22s - loss: 9377.1670"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fc5e46219aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mclassmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mclassmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclassmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mclassification_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXclass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c41b8c8d2451>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Users/chrishealy/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# regmodel = SVI(RegModel, 1)\n",
    "# regmodel.compile(optimizer = tf.keras.optimizers.Adam(lr=0.001))\n",
    "# regmodel.fit(Xreg, Yreg)\n",
    "# print(regmodel.evaluate(Xreg, Yreg)) \n",
    "# regression_dist = regmodel.predict(Xreg[0:5])\n",
    "# print(regression_dist.sample())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classmodel = SVI(ClassModel, 1, task = 'classification', one_hot = True)\n",
    "classmodel.compile(optimizer = tf.keras.optimizers.Adam())\n",
    "classmodel.fit(Xclass, Yclass)\n",
    "print(classmodel.evaluate(Xclass, Yclass))\n",
    "classification_dist = classmodel.predict(Xclass[0:5])\n",
    "print(classification_dist.sample())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xreg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.98208"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "515345/15625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.1709458  -1.0055563   0.60173005]\n",
      " [ 1.1605917  -0.58635294  0.7802697 ]\n",
      " [-0.11709109 -0.33026427  0.42474884]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal([3,3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(tf.clip_by_value((a), 1., 1.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(13948.345, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dim = 7\n",
    "y_pred = tf.ones(7*32)\n",
    "y_obs = tf.ones(7*32)\n",
    "\n",
    "y_pred = tf.reshape(y_pred, [-1, dim])\n",
    "y_obs = tf.reshape(y_obs, [-1, dim, 1])\n",
    "dist = tfp.distributions.Categorical(logits = y_pred)\n",
    "print(K.sum(-dist.log_prob(y_obs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
